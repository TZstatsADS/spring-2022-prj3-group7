{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5691318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras as K\n",
    "import cv2\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import PIL\n",
    "from datetime import datetime\n",
    "import collections\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56d1b61",
   "metadata": {},
   "source": [
    "## 1. Load the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a91bb1",
   "metadata": {},
   "source": [
    "For the project, we provide a training set with 50000 images in the directory `../data/images/` with:\n",
    "- noisy labels for all images provided in `../data/noisy_label.csv`;\n",
    "- clean labels for the first 10000 images provided in `../data/clean_labels.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef3b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DO NOT MODIFY THIS CELL]\n",
    "\n",
    "# load the images\n",
    "n_img = 50000\n",
    "n_noisy = 40000\n",
    "n_clean_noisy = n_img - n_noisy\n",
    "imgs = np.empty((n_img,32,32,3))\n",
    "for i in range(n_img):\n",
    "    img_fn = f'../data/images/{i+1:05d}.png'\n",
    "    imgs[i,:,:,:]=cv2.cvtColor(cv2.imread(img_fn),cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# load the labels\n",
    "clean_labels = np.genfromtxt('../data/clean_labels.csv', delimiter=',', dtype=\"int8\")\n",
    "noisy_labels = np.genfromtxt('../data/noisy_labels.csv', delimiter=',', dtype=\"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f7c35",
   "metadata": {},
   "source": [
    "For illustration, we present a small subset (of size 8) of the images with their clean and noisy labels in `clean_noisy_trainset`. You are encouraged to explore more characteristics of the label noises on the whole dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f48ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DO NOT MODIFY THIS CELL]\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(2,4,1)\n",
    "ax1.imshow(imgs[0]/255)\n",
    "ax2 = fig.add_subplot(2,4,2)\n",
    "ax2.imshow(imgs[1]/255)\n",
    "ax3 = fig.add_subplot(2,4,3)\n",
    "ax3.imshow(imgs[2]/255)\n",
    "ax4 = fig.add_subplot(2,4,4)\n",
    "ax4.imshow(imgs[3]/255)\n",
    "ax1 = fig.add_subplot(2,4,5)\n",
    "ax1.imshow(imgs[4]/255)\n",
    "ax2 = fig.add_subplot(2,4,6)\n",
    "ax2.imshow(imgs[5]/255)\n",
    "ax3 = fig.add_subplot(2,4,7)\n",
    "ax3.imshow(imgs[6]/255)\n",
    "ax4 = fig.add_subplot(2,4,8)\n",
    "ax4.imshow(imgs[7]/255)\n",
    "\n",
    "# The class-label correspondence\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# print clean labels\n",
    "print('Clean labels:')\n",
    "print(' '.join('%5s' % classes[clean_labels[j]] for j in range(10)))\n",
    "# print noisy labels\n",
    "print('Noisy labels:')\n",
    "print(' '.join('%5s' % classes[noisy_labels[j]] for j in range(10)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7004ce2",
   "metadata": {},
   "source": [
    "## 2. The predictive model\n",
    "\n",
    "We consider a baseline model directly on the noisy dataset without any label corrections. RGB histogram features are extracted to fit a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173df0e7",
   "metadata": {},
   "source": [
    "### 2.1. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DO NOT MODIFY THIS CELL]\n",
    "# RGB histogram dataset construction\n",
    "no_bins = 6\n",
    "bins = np.linspace(0,255,no_bins) # the range of the rgb histogram\n",
    "target_vec = np.empty(n_img)\n",
    "feature_mtx = np.empty((n_img,3*(len(bins)-1)))\n",
    "i = 0\n",
    "for i in range(n_img):\n",
    "    # The target vector consists of noisy labels\n",
    "    target_vec[i] = noisy_labels[i]\n",
    "    \n",
    "    # Use the numbers of pixels in each bin for all three channels as the features\n",
    "    feature1 = np.histogram(imgs[i][:,:,0],bins=bins)[0] \n",
    "    feature2 = np.histogram(imgs[i][:,:,1],bins=bins)[0]\n",
    "    feature3 = np.histogram(imgs[i][:,:,2],bins=bins)[0]\n",
    "    \n",
    "    # Concatenate three features\n",
    "    feature_mtx[i,] = np.concatenate((feature1, feature2, feature3), axis=None)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b995e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DO NOT MODIFY THIS CELL]\n",
    "# Train a logistic regression model \n",
    "clf = LogisticRegression(random_state=0).fit(feature_mtx, target_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187d2e6",
   "metadata": {},
   "source": [
    "For the convenience of evaluation, we write the following function `predictive_model` that does the label prediction. **For your predictive model, feel free to modify the function, but make sure the function takes an RGB image of numpy.array format with dimension $32\\times32\\times3$  as input, and returns one single label as output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a44b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DO NOT MODIFY THIS CELL]\n",
    "def baseline_model(image):\n",
    "    '''\n",
    "    This is the baseline predictive model that takes in the image and returns a label prediction\n",
    "    '''\n",
    "    feature1 = np.histogram(image[:,:,0],bins=bins)[0]\n",
    "    feature2 = np.histogram(image[:,:,1],bins=bins)[0]\n",
    "    feature3 = np.histogram(image[:,:,2],bins=bins)[0]\n",
    "    feature = np.concatenate((feature1, feature2, feature3), axis=None).reshape(1,-1)\n",
    "    return clf.predict(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d893e860",
   "metadata": {},
   "source": [
    "### 2.2 Model I (Treating noisy labels as clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c983b",
   "metadata": {},
   "source": [
    "##### Train-test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d244c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(imgs[:10000], clean_labels, test_size=0.2, random_state=42)\n",
    "#X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train_40k = imgs[10000:] / 255.0\n",
    "y_train_40k = noisy_labels[10000:]\n",
    "\n",
    "X_valid_10k = imgs[:10000] / 255.0\n",
    "y_valid_10k = clean_labels\n",
    "\n",
    "#X_valid, X_test, y_valid, y_test = train_test_split(imgs[:10000], clean_labels, test_size=0.5, random_state=42)\n",
    "#X_valid = np.array(X_valid) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbca5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_40k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86890e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_all = np.concatenate((X_train, X_train_noisy))\n",
    "#y_train_all = np.concatenate((y_train, y_train_noisy))\n",
    "#X_train_all = np.array(X_train_all) / 255.0\n",
    "#X_valid = np.array(X_valid) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf58563",
   "metadata": {},
   "source": [
    "#### 2.2.1 CNN without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6d1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/images/cnn\n",
    "cnn = Sequential()\n",
    "cnn.add(layers.Conv2D(32, (3,3), padding=\"same\", activation=\"relu\", input_shape=(32, 32, 3)))\n",
    "cnn.add(layers.MaxPooling2D(2, 2))\n",
    "cnn.add(layers.Conv2D(64, (3,3), padding=\"same\", activation=\"relu\"))\n",
    "cnn.add(layers.MaxPooling2D(2, 2))\n",
    "cnn.add(layers.Conv2D(64, (3,3), padding=\"same\", activation=\"relu\"))\n",
    "\n",
    "# add dense layers on top\n",
    "cnn.add(layers.Flatten())\n",
    "cnn.add(layers.Dense(64, activation='relu'))\n",
    "cnn.add(layers.Dense(10))\n",
    "\n",
    "# compile the model\n",
    "cnn.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7462de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for autosaving the model with the best validation accuracy\n",
    "# reduce learning rate when val_accuracy has stopped improving\n",
    "lr_reduce = K.callbacks.ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                                          factor=0.6,\n",
    "                                          patience=2,\n",
    "                                          verbose=1,\n",
    "                                          mode='max',\n",
    "                                          min_lr=1e-7)\n",
    "# stop training when val_accuracy has stopped improving\n",
    "early_stop = K.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                       patience=3,\n",
    "                                       verbose=1,\n",
    "                                       mode='max')\n",
    "# callback to save the Keras model and (best) weights obtained on an epoch basis. here, the trained (compiled) model is saved in the current working directory as ''\n",
    "filepath_cnn = '../output/saved_model/cnn'\n",
    "checkpoint = K.callbacks.ModelCheckpoint(filepath=filepath_cnn, # remember to change the path\n",
    "                                         monitor='val_accuracy',\n",
    "                                         verbose=1,\n",
    "                                         save_weights_only=False,\n",
    "                                         save_best_only=True,\n",
    "                                         mode='max',\n",
    "                                         save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32801c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model and autosave the model with the best validation accuracy\n",
    "cnn_history = cnn.fit(X_train_40k,\n",
    "                      y_train_40k, \n",
    "                      epochs = 20, \n",
    "                      validation_data=(X_valid_10k, y_valid_10k),\n",
    "                      shuffle=True,\n",
    "                      callbacks=[lr_reduce, early_stop, checkpoint],\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62747893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fit history\n",
    "import pickle\n",
    "with open('../output/evaluation_model/cnn_hisotry', 'wb') as fp:\n",
    "    pickle.dump(cnn_history, fp)\n",
    "    \n",
    "# Load the history fit back from the pickle file.\n",
    "# import pickle\n",
    "# cnn_history = pickle.load(open(\"../output/evaluation_model/cnn_hisotry\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c715dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "filepath_cnn = '../output/saved_model/cnn'\n",
    "cnn = K.models.load_model(filepath=filepath_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbe9a19",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff2013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. time\n",
    "start = timeit.default_timer()\n",
    "\n",
    "y_pred = cnn.predict(X_valid_10k)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(f'-----The Model takes {stop - start:.2f} seconds to run 10k predictions-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training history\n",
    "# 2. Validation accuracy\n",
    "# plt.plot(cnn_history.history['accuracy'], label = 'accuracy')\n",
    "plt.plot(cnn_history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('../figs/cnn_accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd978063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Validation loss\n",
    "# plt.plot(cnn_history.history['loss'], label='Training Loss')\n",
    "plt.plot(cnn_history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.savefig('../figs/cnn_loss')\n",
    "plt.show()\n",
    "\n",
    "#Load and evaluate the best model version\n",
    "# model = load_model(filepath)\n",
    "# yhat = model.predict(X_test)\n",
    "# print('Model MSE on test data = ', mse(y_test, yhat).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795b1c9d",
   "metadata": {},
   "source": [
    "#### 2.2.2 CNN with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ec381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [BUILD A MORE SOPHISTICATED PREDICTIVE MODEL]\n",
    "\n",
    "# data augomentation\n",
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n",
    "                                                 input_shape=(32, \n",
    "                                                              32,3)),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.1),\n",
    "  ]\n",
    ")\n",
    "\n",
    "\n",
    "cnn_aug = Sequential([\n",
    "  data_augmentation,\n",
    "  layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "# compile the model\n",
    "cnn_aug.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ff739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for autosaving the model with the best validation accuracy\n",
    "filepath_cnn_aug = '../output/saved_model/cnn_aug'\n",
    "# reduce learning rate when val_accuracy has stopped improving\n",
    "lr_reduce = K.callbacks.ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                                          factor=0.6,\n",
    "                                          patience=2,\n",
    "                                          verbose=1,\n",
    "                                          mode='max',\n",
    "                                          min_lr=1e-7)\n",
    "# stop training when val_accuracy has stopped improving\n",
    "early_stop = K.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                       patience=3,\n",
    "                                       verbose=1,\n",
    "                                       mode='max')\n",
    "# callback to save the Keras model and (best) weights obtained on an epoch basis. here, the trained (compiled) model is saved in the current working directory as ''\n",
    "checkpoint = K.callbacks.ModelCheckpoint(filepath=filepath_cnn_aug, # remember to change the path\n",
    "                                         monitor='val_accuracy',\n",
    "                                         verbose=1,\n",
    "                                         save_weights_only=False,\n",
    "                                         save_best_only=True,\n",
    "                                         mode='max',\n",
    "                                         save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0738efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model and autosave the model with the best validation accuracy\n",
    "cnn_aug_history = cnn_aug.fit(X_train_40k,\n",
    "                      y_train_40k, \n",
    "                      epochs = 20, \n",
    "                      validation_data=(X_valid_10k, y_valid_10k),\n",
    "                      shuffle=True,\n",
    "                      callbacks=[lr_reduce, early_stop, checkpoint],\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10968454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fit history\n",
    "import pickle\n",
    "with open('../output/evaluation_model/cnn_aug_hisotry', 'wb') as fp:\n",
    "    pickle.dump(cnn_aug_history, fp)\n",
    "    \n",
    "# Load the history fit back from the pickle file.\n",
    "# import pickle\n",
    "# cnn_history = pickle.load(open(\"../output/evaluation_model/cnn_aug_hisotry\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae752ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "filepath_cnn_aug = '../output/saved_model/cnn_aug'\n",
    "cnn_aug = K.models.load_model(filepath=filepath_cnn_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9510293a",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc0d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. time\n",
    "start = timeit.default_timer()\n",
    "\n",
    "y_pred = cnn_aug.predict(X_valid_10k)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(f'-----The Model takes {stop - start:.2f} seconds to run 10k predictions-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aec282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training history\n",
    "# 2. Validation accuracy\n",
    "# plt.plot(cnn_aug_history.history['accuracy'], label = 'accuracy')\n",
    "plt.plot(cnn_aug_history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('../figs/cnn_aug_accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add3180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Validation loss\n",
    "# plt.plot(cnn_aug_history.history['loss'], label='Training Loss')\n",
    "plt.plot(cnn_aug_history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.savefig('../figs/cnn_aug_loss')\n",
    "plt.show()\n",
    "\n",
    "#Load and evaluate the best model version\n",
    "# model = load_model(filepath)\n",
    "# yhat = model.predict(X_test)\n",
    "# print('Model MSE on test data = ', mse(y_test, yhat).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d541ac3",
   "metadata": {},
   "source": [
    "After comparing the performance of CNN without data augmentation and CNN without data augmentation, we choose CNN with data augmentation as our Model I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c1467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_I(image):\n",
    "#      '''\n",
    "#     This function should takes in the image of dimension 32*32*3 as input and returns a label prediction\n",
    "#     '''\n",
    "    X = image/255.0\n",
    "    # predict\n",
    "    y_pred = cnn_aug.predict(X)\n",
    "    y_pred_labels = [np.argmax(x) for x in y_pred]\n",
    "    return y_pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2764b6",
   "metadata": {},
   "source": [
    "### 2.3. Model II (After cleaning the labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c45ee7d",
   "metadata": {},
   "source": [
    "#### Correct the noisy labels\n",
    "We put the code in the '../doc/label_correction_model.ipynb' to keep this notebook tidy.\n",
    "\n",
    "After the model is created to clean the labels, cleaned labels are saved in output, and they are used in the below model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb14d18",
   "metadata": {},
   "source": [
    "#### Load the data (cleaned labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b6c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_labels = np.genfromtxt('../output/cleaned_labels.csv', delimiter=',', dtype=\"int8\")\n",
    "y_train_cleaned = cleaned_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14319bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [BUILD A MORE SOPHISTICATED PREDICTIVE MODEL]\n",
    "\n",
    "# data augomentation\n",
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n",
    "                                                 input_shape=(32, \n",
    "                                                              32,3)),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.1),\n",
    "  ]\n",
    ")\n",
    "\n",
    "\n",
    "model_2 = Sequential([\n",
    "  data_augmentation,\n",
    "  layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "# compile the model\n",
    "model_2.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d46370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for autosaving the model with the best validation accuracy\n",
    "filepath_model_2 = '../output/saved_model/model_2'\n",
    "# reduce learning rate when val_accuracy has stopped improving\n",
    "lr_reduce = K.callbacks.ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                                          factor=0.6,\n",
    "                                          patience=2,\n",
    "                                          verbose=1,\n",
    "                                          mode='max',\n",
    "                                          min_lr=1e-7)\n",
    "# stop training when val_accuracy has stopped improving\n",
    "early_stop = K.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                       patience=3,\n",
    "                                       verbose=1,\n",
    "                                       mode='max')\n",
    "# callback to save the Keras model and (best) weights obtained on an epoch basis. here, the trained (compiled) model is saved in the current working directory as ''\n",
    "checkpoint = K.callbacks.ModelCheckpoint(filepath=filepath_model_2, # remember to change the path\n",
    "                                         monitor='val_accuracy',\n",
    "                                         verbose=1,\n",
    "                                         save_weights_only=False,\n",
    "                                         save_best_only=True,\n",
    "                                         mode='max',\n",
    "                                         save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db7e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model and autosave the model with the best validation accuracy\n",
    "model_2_history = model_2.fit(X_train_40k,\n",
    "                      y_train_cleaned, # use the corrected labels to train the data\n",
    "                      epochs = 20, \n",
    "                      validation_data=(X_valid_10k, y_valid_10k),\n",
    "                      shuffle=True,\n",
    "                      callbacks=[lr_reduce, early_stop, checkpoint],\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3feb903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fit history\n",
    "import pickle\n",
    "with open('../output/evaluation_model/model_2_hisotry', 'wb') as fp:\n",
    "    pickle.dump(model_2_history, fp)\n",
    "    \n",
    "# Load the history fit back from the pickle file.\n",
    "# import pickle\n",
    "# cnn_history = pickle.load(open(\"../output/evaluation_model/cnn_aug_hisotry\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb28249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "filepath_model_2 = '../output/saved_model/model_2'\n",
    "cnn_aug = K.models.load_model(filepath=filepath_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce6cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. time\n",
    "start = timeit.default_timer()\n",
    "\n",
    "y_pred = model_2.predict(X_valid_10k)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(f'-----The Model takes {stop - start:.2f} seconds to run 10k predictions-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3598ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training history\n",
    "# 2. Validation accuracy\n",
    "# plt.plot(cnn_aug_history.history['accuracy'], label = 'accuracy')\n",
    "plt.plot(model_2_history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('../figs/model_2_accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e098625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Validation loss\n",
    "# plt.plot(cnn_aug_history.history['loss'], label='Training Loss')\n",
    "plt.plot(model_2_history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.savefig('../figs/model_2_loss')\n",
    "plt.show()\n",
    "\n",
    "#Load and evaluate the best model version\n",
    "# model = load_model(filepath)\n",
    "# yhat = model.predict(X_test)\n",
    "# print('Model MSE on test data = ', mse(y_test, yhat).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e17cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_II(image):\n",
    "#      '''\n",
    "#     This function should takes in the image of dimension 32*32*3 as input and returns a label prediction\n",
    "#     '''\n",
    "    X = image/255.0\n",
    "    # predict\n",
    "    y_pred = model_2.predict(X)\n",
    "    y_pred_labels = [np.argmax(x) for x in y_pred]\n",
    "    return y_pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdbe04d",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae68748f",
   "metadata": {},
   "source": [
    "For assessment, we will evaluate your final model on a hidden test dataset with clean labels by the `evaluation` function defined as follows. Although you will not have the access to the test set, the function would be useful for the model developments. For example, you can split the small training set, using one portion for weakly supervised learning and the other for validation purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cd8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model 1\n",
    "filepath_cnn_aug = '../output/saved_model/cnn_aug'\n",
    "cnn_aug = K.models.load_model(filepath=filepath_cnn_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cc6c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model 2\n",
    "filepath_model_2 = '../output/saved_model/model_2'\n",
    "model_2 = K.models.load_model(filepath=filepath_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942719f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_I(image):\n",
    "#      '''\n",
    "#     This function should takes in the image of dimension 32*32*3 as input and returns a label prediction\n",
    "#     '''\n",
    "    X = image/255.0\n",
    "    # predict\n",
    "    y_pred = cnn_aug.predict(X)\n",
    "    y_pred_labels = [np.argmax(x) for x in y_pred]\n",
    "    return y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01da5c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_II(image):\n",
    "#      '''\n",
    "#     This function should takes in the image of dimension 32*32*3 as input and returns a label prediction\n",
    "#     '''\n",
    "    X = image/255.0\n",
    "    # predict\n",
    "    y_pred = model_2.predict(X)\n",
    "    y_pred_labels = [np.argmax(x) for x in y_pred]\n",
    "    return y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe760df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [DO NOT MODIFY THIS CELL]\n",
    "def evaluation(model, test_labels, test_imgs):\n",
    "    y_true = test_labels.tolist()\n",
    "    y_pred = model(test_imgs)\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4665b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For final test\n",
    "# [DO NOT MODIFY THIS CELL]\n",
    "# This is the code for evaluating the prediction performance on a testset\n",
    "# You will get an error if running this cell, as you do not have the testset\n",
    "# Nonetheless, you can create your own validation set to run the evlauation\n",
    "# n_test = 10000\n",
    "# test_labels = np.genfromtxt('../data/test_labels.csv', delimiter=',', dtype=\"int8\")\n",
    "# test_imgs = np.empty((n_test,32,32,3))\n",
    "# for i in range(n_test):\n",
    "#     img_fn = f'../data/test_images/test{i+1:05d}.png'\n",
    "#     test_imgs[i,:,:,:]=cv2.cvtColor(cv2.imread(img_fn),cv2.COLOR_BGR2RGB)\n",
    "# evaluation(baseline_model, test_labels, test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b02112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just for our trial, remember to delete this cell\n",
    "n_test = 10000\n",
    "test_labels = np.genfromtxt('../data/clean_labels.csv', delimiter=',', dtype=\"int8\") \n",
    "test_imgs = np.empty((n_test,32,32,3))\n",
    "for i in range(n_test):\n",
    "    img_fn = f'../data/images/{i+1:05d}.png'\n",
    "    test_imgs[i,:,:,:]=cv2.cvtColor(cv2.imread(img_fn),cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b63426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for CNN with augmentation (Model_I)\n",
    "start = timeit.default_timer()\n",
    "\n",
    "evaluation(model_I, test_labels, test_imgs)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(f'-----The Model I takes {stop - start:.2f} seconds-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36673d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for CNN with augmentation (Model_I)\n",
    "start = timeit.default_timer()\n",
    "\n",
    "evaluation(model_II, test_labels, test_imgs)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(f'-----The Model II takes {stop - start:.2f} seconds-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c204b",
   "metadata": {},
   "source": [
    "The overall accuracy is $0.24$, which is better than random guess (which should have a accuracy around $0.10$). For the project, you should try to improve the performance by the following strategies:\n",
    "\n",
    "- Consider a better choice of model architectures, hyperparameters, or training scheme for the predictive model;\n",
    "- Use both `clean_noisy_trainset` and `noisy_trainset` for model training via **weakly supervised learning** methods. One possible solution is to train a \"label-correction\" model using the former, correct the labels in the latter, and train the final predictive model using the corrected dataset.\n",
    "- Apply techniques such as $k$-fold cross validation to avoid overfitting;\n",
    "- Any other reasonable strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
